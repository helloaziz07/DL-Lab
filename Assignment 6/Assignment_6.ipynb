{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28c00b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import pandas as pd    # for loading and handling dataset\n",
    "import numpy as np     # for mathematical operations\n",
    "import re              # for regex text cleaning\n",
    "from nltk.corpus import stopwords   # stopwords for text preprocessing\n",
    "from sklearn.model_selection import train_test_split       # split dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # encode text to integers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # padding/truncating\n",
    "from tensorflow.keras.models import Sequential     # sequential model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # model layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint     # save best model\n",
    "from tensorflow.keras.models import load_model             # load saved model\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('Dataset/IMDB Dataset.csv')\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "641af810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample reviews:\n",
      " 0    [one, reviewers, mentioned, watching, oz, epis...\n",
      "1    [a, wonderful, little, production, the, filmin...\n",
      "2    [i, thought, wonderful, way, spend, time, hot,...\n",
      "3    [basically, family, little, boy, jake, thinks,...\n",
      "4    [petter, mattei, love, time, money, visually, ...\n",
      "Name: review, dtype: object\n",
      "\n",
      "Sample sentiments:\n",
      " 0    1\n",
      "1    1\n",
      "2    1\n",
      "3    0\n",
      "4    1\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_6460\\845340564.py:25: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_data = y_data.replace('negative', 0)\n"
     ]
    }
   ],
   "source": [
    "# Define English stopwords\n",
    "\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "# Function to load and preprocess dataset\n",
    "def load_dataset():\n",
    "    df = pd.read_csv('Dataset/IMDB Dataset.csv')\n",
    "    x_data = df['review']       # reviews (input)\n",
    "    y_data = df['sentiment']    # sentiment (output)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex=True)\n",
    "\n",
    "    # Remove non-alphabet characters\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex=True)\n",
    "\n",
    "    # Remove stopwords\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])\n",
    "\n",
    "    # Convert to lowercase\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])\n",
    "\n",
    "    # Encode labels (positive → 1, negative → 0)\n",
    "    y_data = y_data.replace('positive', 1)\n",
    "    y_data = y_data.replace('negative', 0)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "# Load preprocessed data\n",
    "x_data, y_data = load_dataset()\n",
    "\n",
    "print(\"Sample reviews:\\n\", x_data.head())\n",
    "print(\"\\nSample sentiments:\\n\", y_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5858f566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 40000\n",
      "Test size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Train Test Split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_data, y_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(x_train))\n",
    "print(\"Test size:\", len(x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "294549d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 130\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate max review length (average)\n",
    "\n",
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "    return int(np.ceil(np.mean(review_length)))\n",
    "\n",
    "max_length = get_max_length()\n",
    "print(\"Maximum review length:\", max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b8f21ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 92546\n",
      "Example padded review:\n",
      " [  145     1   702  2078    38  1815  1945  4247  6378   698  4753 21764\n",
      "   135     2  6062    22   680    30     5  1922    31    48  1031  2286\n",
      "  1095   361   458   111   820   309 28145    70  2900 35698  4284  2861\n",
      "   129    17    53  3064   141   339     1   110  4855     1    14     5\n",
      " 14365    18   269     4   110   212 56913 39814  1772  9974    45   324\n",
      " 45914 45915  1598 11127   480   330   286     1    46    44   272 11735\n",
      "     1     1  6717   268 12160  1076 23833   969 56914 56915   373   125\n",
      "   920  4090    86   345   598   525   143    41   750   287   128   335\n",
      "   271    10  3660   184 10125    73  1619    98 11519 11324   629  1859\n",
      "  1110  2803 11325  3635  1599 32614   379     1   136   115   231     1\n",
      "   980  2755  8590     1  9082  3940     3   903 16627   401]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize words\n",
    "token = Tokenizer(lower=False)    # no need lower, already lowercase\n",
    "token.fit_on_texts(x_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "# Pad sequences to fixed length\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Total words in vocabulary\n",
    "total_words = len(token.word_index) + 1   # +1 for padding\n",
    "\n",
    "print(\"Vocabulary size:\", total_words)\n",
    "print(\"Example padded review:\\n\", x_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a62f125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aziz\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,961,472</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m2,961,472\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m24,832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,986,369</span> (11.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,986,369\u001b[0m (11.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,986,369</span> (11.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,986,369\u001b[0m (11.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Model architecture\n",
    "\n",
    "EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length=max_length))\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "model.add(Dense(1, activation='sigmoid'))  # binary output\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.build(input_shape=(None, max_length))\n",
    "\n",
    "# Show model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "773cb07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.5124 - loss: 0.6921\n",
      "Epoch 1: accuracy improved from -inf to 0.51858, saving model to models/LSTM.keras\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 119ms/step - accuracy: 0.5124 - loss: 0.6921\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.6485 - loss: 0.6326\n",
      "Epoch 2: accuracy improved from 0.51858 to 0.68415, saving model to models/LSTM.keras\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 110ms/step - accuracy: 0.6486 - loss: 0.6325\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.6396 - loss: 0.6211\n",
      "Epoch 3: accuracy did not improve from 0.68415\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 117ms/step - accuracy: 0.6396 - loss: 0.6212\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.7254 - loss: 0.5749\n",
      "Epoch 4: accuracy improved from 0.68415 to 0.73153, saving model to models/LSTM.keras\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 114ms/step - accuracy: 0.7255 - loss: 0.5748\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8056 - loss: 0.4471\n",
      "Epoch 5: accuracy improved from 0.73153 to 0.79240, saving model to models/LSTM.keras\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 110ms/step - accuracy: 0.8056 - loss: 0.4471\n"
     ]
    }
   ],
   "source": [
    "# Save best model \n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'models/LSTM.keras',\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=5,\n",
    "    callbacks=[checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "639c6d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step\n",
      "Correct Predictions: 7642\n",
      "Wrong Predictions: 2358\n",
      "Accuracy: 76.42\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_pred = (model.predict(x_test, batch_size=128) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate accuracy manually\n",
    "true = np.sum(y_test.values == y_pred.flatten())\n",
    "print(\"Correct Predictions:\", true)\n",
    "print(\"Wrong Predictions:\", len(y_pred) - true)\n",
    "print(\"Accuracy:\", true / len(y_pred) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dd2cf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned: I Love Mountains\n",
      "Filtered: ['i love mountains']\n",
      "Encoded input: [[   1   42 4076    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n",
      "Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "# Load best saved model\n",
    "from tensorflow.keras.models import load_model  \n",
    "loaded_model = load_model('models/LSTM.keras')\n",
    "\n",
    "# Input custom review\n",
    "review = str(input(\"Movie Review: \"))\n",
    "\n",
    "# Clean input review\n",
    "regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "review = regex.sub('', review)\n",
    "print(\"Cleaned:\", review)\n",
    "\n",
    "# Remove stopwords\n",
    "words = review.split(' ')\n",
    "filtered = [w for w in words if w not in english_stops]\n",
    "filtered = ' '.join(filtered)\n",
    "filtered = [filtered.lower()]\n",
    "print(\"Filtered:\", filtered)\n",
    "\n",
    "# Convert to sequence\n",
    "tokenize_words = token.texts_to_sequences(filtered)\n",
    "tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
    "print(\"Encoded input:\", tokenize_words)\n",
    "\n",
    "# Predict sentiment\n",
    "result = loaded_model.predict(tokenize_words)\n",
    "\n",
    "if result >= 0.7:\n",
    "    print(\"Sentiment: Positive\")\n",
    "else:\n",
    "    print(\"Sentiment: Negative\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
